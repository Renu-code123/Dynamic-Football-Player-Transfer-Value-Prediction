{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8259dc4f",
   "metadata": {},
   "source": [
    "# WITH 174 FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdddb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1885128, 174)\n",
      "y_train shape: (1885128, 1)\n",
      "X_test shape: (471282, 174)\n",
      "y_test shape: (471282, 1)\n",
      "[0]\teval-rmse:0.90631\n",
      "[50]\teval-rmse:0.18163\n",
      "[100]\teval-rmse:0.16842\n",
      "[150]\teval-rmse:0.16034\n",
      "[200]\teval-rmse:0.15391\n",
      "[250]\teval-rmse:0.14942\n",
      "[300]\teval-rmse:0.14568\n",
      "[350]\teval-rmse:0.14207\n",
      "[400]\teval-rmse:0.13901\n",
      "[450]\teval-rmse:0.13624\n",
      "[499]\teval-rmse:0.13380\n",
      "\n",
      "----- RESULTS -----\n",
      "MSE  : 0.017902156338095665\n",
      "RMSE : 0.13379893997373696\n",
      "R²   : 0.9821070432662964\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Load TRAIN features in chunks\n",
    "# Example: combining poly_train_part1 + poly_train_part2 with other_train\n",
    "poly_train_part1 = pd.read_csv(\"poly_train_part1.csv\")\n",
    "poly_train_part2 = pd.read_csv(\"poly_train_part2.csv\")\n",
    "other_train = pd.read_csv(\"other_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "\n",
    "# Combine polynomial parts\n",
    "poly_train_full = pd.concat([poly_train_part1, poly_train_part2], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Combine with other_train\n",
    "X_train = pd.concat([poly_train_full, other_train], axis=1)\n",
    "\n",
    "# Clean target\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# 2. Load TEST set\n",
    "poly_test = pd.read_csv(\"poly_test.csv\")\n",
    "other_test = pd.read_csv(\"other_test.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\").reset_index(drop=True)\n",
    "\n",
    "X_test = pd.concat([poly_test, other_test], axis=1)\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# 3. Convert to DMatrix (XGBoost format)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# 4. Train XGBoost Regressor\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",   # memory-efficient for large data\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1\n",
    "}\n",
    "\n",
    "num_round = 500\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_boost_round=num_round, evals=[(dtest, \"eval\")], verbose_eval=50)\n",
    "\n",
    "# 5. Make Predictions & Evaluate\n",
    "y_pred = bst.predict(dtest)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse**0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n----- RESULTS -----\")\n",
    "print(\"MSE  :\", mse)\n",
    "print(\"RMSE :\", rmse)\n",
    "print(\"R²   :\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b30ca",
   "metadata": {},
   "source": [
    "# WITH 20 FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cf553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1885128, 20)\n",
      "y_train shape: (1885128,)\n",
      "X_test shape: (471282, 20)\n",
      "y_test shape: (471282,)\n",
      "[0]\ttrain-rmse:0.90617\ttest-rmse:0.90628\n",
      "[50]\ttrain-rmse:0.18378\ttest-rmse:0.18461\n",
      "[100]\ttrain-rmse:0.16889\ttest-rmse:0.17024\n",
      "[150]\ttrain-rmse:0.15999\ttest-rmse:0.16176\n",
      "[200]\ttrain-rmse:0.15454\ttest-rmse:0.15648\n",
      "[250]\ttrain-rmse:0.14928\ttest-rmse:0.15142\n",
      "[300]\ttrain-rmse:0.14567\ttest-rmse:0.14785\n",
      "[350]\ttrain-rmse:0.14221\ttest-rmse:0.14448\n",
      "[400]\ttrain-rmse:0.13909\ttest-rmse:0.14143\n",
      "[450]\ttrain-rmse:0.13646\ttest-rmse:0.13892\n",
      "[499]\ttrain-rmse:0.13374\ttest-rmse:0.13626\n",
      "\n",
      "----- TRAIN RESULTS (XGBoost) -----\n",
      "MSE  : 0.017886225221657375\n",
      "RMSE : 0.13373939293139242\n",
      "R²   : 0.9821199711750015\n",
      "\n",
      "----- TEST RESULTS (XGBoost) -----\n",
      "MSE  : 0.01856751469783279\n",
      "RMSE : 0.13626266802698672\n",
      "R²   : 0.9814420164498081\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. LOAD TRAIN & TEST DATA (TOP FEATURES, MEMORY OPTIMIZED)\n",
    "X_train = pd.read_csv(\"X_train_top_features.csv\", dtype=np.float32)\n",
    "X_test = pd.read_csv(\"X_test_top_features.csv\", dtype=np.float32)\n",
    "\n",
    "y_train = pd.read_csv(\"y_train.csv\").values.ravel()\n",
    "y_test = pd.read_csv(\"y_test.csv\").values.ravel()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# 2. CONVERT TO DMatrix (XGBoost format)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# 3. TRAIN XGBOOST REGRESSOR (FAST + MEMORY EFFICIENT)\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",     # best for large data on CPU\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "    \"subsample\": 0.7,          # row subsampling\n",
    "    \"colsample_bytree\": 0.7,   # feature subsampling\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "num_rounds = 500\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_rounds,\n",
    "    evals=[(dtrain, \"train\"), (dtest, \"test\")],\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# 4. EVALUATE ON TRAIN DATA\n",
    "y_train_pred = bst.predict(dtrain)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"\\n----- TRAIN RESULTS (XGBoost) -----\")\n",
    "print(\"MSE  :\", mse_train)\n",
    "print(\"RMSE :\", rmse_train)\n",
    "print(\"R²   :\", r2_train)\n",
    "\n",
    "# 5. EVALUATE ON TEST DATA\n",
    "y_test_pred = bst.predict(dtest)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n----- TEST RESULTS (XGBoost) -----\")\n",
    "print(\"MSE  :\", mse_test)\n",
    "print(\"RMSE :\", rmse_test)\n",
    "print(\"R²   :\", r2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b646533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1885128, 20)\n",
      "y_train shape: (1885128,)\n",
      "X_test shape : (471282, 20)\n",
      "y_test shape : (471282,)\n",
      "[0]\ttrain-rmse:0.90632\tvalidation-rmse:0.90548\n",
      "[50]\ttrain-rmse:0.18308\tvalidation-rmse:0.18380\n",
      "[100]\ttrain-rmse:0.16806\tvalidation-rmse:0.16895\n",
      "[150]\ttrain-rmse:0.16008\tvalidation-rmse:0.16123\n",
      "[200]\ttrain-rmse:0.15396\tvalidation-rmse:0.15538\n",
      "[250]\ttrain-rmse:0.14913\tvalidation-rmse:0.15077\n",
      "[300]\ttrain-rmse:0.14517\tvalidation-rmse:0.14713\n",
      "[350]\ttrain-rmse:0.14176\tvalidation-rmse:0.14395\n",
      "[400]\ttrain-rmse:0.13855\tvalidation-rmse:0.14097\n",
      "[450]\ttrain-rmse:0.13590\tvalidation-rmse:0.13847\n",
      "[499]\ttrain-rmse:0.13352\tvalidation-rmse:0.13615\n",
      "\n",
      "----- VALIDATION RESULTS (XGBoost) -----\n",
      "MSE  : 0.018536195158958435\n",
      "RMSE : 0.13614769612064112\n",
      "R²   : 0.981441855430603\n",
      "\n",
      "----- TEST RESULTS (FINAL - XGBoost) -----\n",
      "MSE  : 0.018638448789715767\n",
      "RMSE : 0.13652270430121052\n",
      "R²   : 0.9813711047172546\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Load Data (same as yours)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load top-20 feature datasets\n",
    "X_train = pd.read_csv(\"X_train_top_features.csv\", dtype=np.float32)\n",
    "X_test  = pd.read_csv(\"X_test_top_features.csv\", dtype=np.float32)\n",
    "\n",
    "y_train = pd.read_csv(\"y_train.csv\", dtype=np.float32).values.ravel()\n",
    "y_test  = pd.read_csv(\"y_test.csv\", dtype=np.float32).values.ravel()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape :\", X_test.shape)\n",
    "print(\"y_test shape :\", y_test.shape)\n",
    "\n",
    "# STEP 2: Create VALIDATION SPLIT (from training only)\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# STEP 3: Convert to DMatrix (XGBoost format)\n",
    "dtrain = xgb.DMatrix(X_train_split, label=y_train_split)\n",
    "dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest  = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# STEP 4: Train XGBoost Model (WITH validation)\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",     # fast for large CPU datasets\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "num_rounds = 500\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_rounds,\n",
    "    evals=[(dtrain, \"train\"), (dval, \"validation\")],\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# STEP 5: Validation Evaluation\n",
    "y_val_pred = bst.predict(dval)\n",
    "\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\n----- VALIDATION RESULTS (XGBoost) -----\")\n",
    "print(\"MSE  :\", mse_val)\n",
    "print(\"RMSE :\", rmse_val)\n",
    "print(\"R²   :\", r2_val)\n",
    "\n",
    "# STEP 6: FINAL TEST EVALUATION (ONLY ONCE)\n",
    "y_test_pred = bst.predict(dtest)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n----- TEST RESULTS (FINAL - XGBoost) -----\")\n",
    "print(\"MSE  :\", mse_test)\n",
    "print(\"RMSE :\", rmse_test)\n",
    "print(\"R²   :\", r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f3da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1885128, 20)\n",
      "y_train: (1885128,)\n",
      "\n",
      "--- Fold 1 ---\n",
      "Fold 1 R²: 0.981406\n",
      "\n",
      "--- Fold 2 ---\n",
      "Fold 2 R²: 0.981960\n",
      "\n",
      "--- Fold 3 ---\n",
      "Fold 3 R²: 0.981950\n",
      "\n",
      "--- Fold 4 ---\n",
      "Fold 4 R²: 0.981658\n",
      "\n",
      "--- Fold 5 ---\n",
      "Fold 5 R²: 0.981849\n",
      "\n",
      "===== 5-FOLD CROSS-VALIDATION RESULTS =====\n",
      "R² scores per fold: [0.98140568 0.98196036 0.9819504  0.98165828 0.9818486 ]\n",
      "Mean R²          : 0.9817646622657776\n",
      "Std Deviation R² : 0.00020977577246453737\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Imports & Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load training data ONLY (top-20 features)\n",
    "X_train = pd.read_csv(\"X_train_top_features.csv\", dtype=np.float32)\n",
    "y_train = pd.read_csv(\"y_train.csv\", dtype=np.float32).values.ravel()\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "\n",
    "# STEP 2: Define 5-Fold Cross-Validation\n",
    "kf = KFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# STEP 3: XGBoost Parameters (same as your best model)\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "num_rounds = 500\n",
    "\n",
    "# STEP 4: Run 5-Fold CV and Collect R² Scores\n",
    "r2_scores = []\n",
    "\n",
    "fold = 1\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # Convert to DMatrix\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_rounds,\n",
    "        evals=[(dval, \"validation\")],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Predict on validation fold\n",
    "    y_val_pred = model.predict(dval)\n",
    "\n",
    "    # Compute R²\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "    print(f\"Fold {fold} R²: {r2:.6f}\")\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# STEP 5: Show Stability of R²\n",
    "r2_scores = np.array(r2_scores)\n",
    "\n",
    "print(\"\\n===== 5-FOLD CROSS-VALIDATION RESULTS =====\")\n",
    "print(\"R² scores per fold:\", r2_scores)\n",
    "print(\"Mean R²          :\", r2_scores.mean())\n",
    "print(\"Std Deviation R² :\", r2_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64902c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1885128, 20)\n",
      "y_train: (1885128,)\n",
      "X_test : (471282, 20)\n",
      "y_test : (471282,)\n",
      "\n",
      "===== FINAL TEST RESULTS =====\n",
      "MSE  : 0.018567511811852455\n",
      "RMSE : 0.13626265743721738\n",
      "R²   : 0.9814420342445374\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Imports & Load Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load top-20 feature training and test data\n",
    "X_train = pd.read_csv(\"X_train_top_features.csv\", dtype=np.float32)\n",
    "y_train = pd.read_csv(\"y_train.csv\", dtype=np.float32).values.ravel()\n",
    "\n",
    "X_test = pd.read_csv(\"X_test_top_features.csv\", dtype=np.float32)\n",
    "y_test = pd.read_csv(\"y_test.csv\", dtype=np.float32).values.ravel()\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test :\", X_test.shape)\n",
    "print(\"y_test :\", y_test.shape)\n",
    "\n",
    "# STEP 2: XGBoost Parameters (same as used in CV)\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "num_rounds = 500\n",
    "\n",
    "# STEP 3: Convert full training data to DMatrix\n",
    "dtrain_full = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# STEP 4: Train Final Model on Full Training Data\n",
    "final_model = xgb.train(\n",
    "    params,\n",
    "    dtrain_full,\n",
    "    num_boost_round=num_rounds,\n",
    "    verbose_eval=50  # optional, shows progress every 50 rounds\n",
    ")\n",
    "\n",
    "# STEP 5: Predict on Test Data\n",
    "y_test_pred = final_model.predict(dtest)\n",
    "\n",
    "# STEP 6: Evaluate Test Performance\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n===== FINAL TEST RESULTS =====\")\n",
    "print(\"MSE  :\", mse_test)\n",
    "print(\"RMSE :\", rmse_test)\n",
    "print(\"R²   :\", r2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a4dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully as 'xgb_final_model.json'\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: Save model in XGBoost native JSON format\n",
    "final_model.save_model(\"xgb_final_model.json\")\n",
    "\n",
    "print(\"Model saved successfully as 'xgb_final_model.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6591c2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'xgb_final_model_joblib.pkl' using joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained XGBoost booster\n",
    "joblib.dump(final_model, \"xgb_final_model_joblib.pkl\")\n",
    "print(\"Model saved as 'xgb_final_model_joblib.pkl' using joblib\")\n",
    "\n",
    "# Later, to load it:\n",
    "# loaded_model = joblib.load(\"xgb_final_model_joblib.pkl\")\n",
    "# y_test_pred = loaded_model.predict(dtest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
